{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing DoRA: Weight-Decomposed Low-Rank Adaptation\n",
        "\n",
        "In this notebook we will be implementing the [Weight-Decomposed Low-Rank Adaptation (DoRA)](https://arxiv.org/abs/2402.09353) architecture, proposed by researchers as a technique that outperforms LoRA by a large margin.\n",
        "\n",
        "To follow along conceptually, you can refer to the [writeup](https://medium.com/p/f814ba519af4/edit) where we go over the theoretical concepts and the motivation behind LoRA and DoRA."
      ],
      "metadata": {
        "id": "HdGG50xVtQuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries"
      ],
      "metadata": {
        "id": "cOqJyjurudcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "x7B7M4youSyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Settings and dataset"
      ],
      "metadata": {
        "id": "3UA8qJgSuvYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = datasets.MNIST(root='data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='data/',\n",
        "                               train=False,\n",
        "                               transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         shuffle=False)\n",
        "\n",
        "for images, labels in train_loader:\n",
        "    print('Image batch dimensions:', images.shape)\n",
        "    print('Image label dimensions:', labels.shape)\n",
        "    break # Only print one, all will have the same dimensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khyuuPcpuhGi",
        "outputId": "fced866e-344c-471c-d30d-cb09acdfbb76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch dimensions: torch.Size([64, 1, 28, 28])\n",
            "Image label dimensions: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multilayer Perceptron Model"
      ],
      "metadata": {
        "id": "i7xVGYVhJqfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 123\n",
        "learning_rate = 0.005\n",
        "num_epochs = 10\n",
        "\n",
        "num_features = 784\n",
        "num_hidden_1 = 128\n",
        "num_hidden_2 = 256\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "\n",
        "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(num_features, num_hidden_1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(num_hidden_1, num_hidden_2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(num_hidden_2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "torch.manual_seed(random_seed)\n",
        "model_pretrained = MultilayerPerceptron(\n",
        "    num_features=num_features,\n",
        "    num_hidden_1=num_hidden_1,\n",
        "    num_hidden_2=num_hidden_2,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "model_pretrained.to(DEVICE)\n",
        "optimizer_pretrained = torch.optim.Adam(model_pretrained.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "kXWETd0qvHTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for features, targets in data_loader: # Processing batches\n",
        "            features = features.view(-1, 28*28).to(device)\n",
        "            targets = targets.to(device)\n",
        "            logits = model(features)\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "            num_examples += targets.size(0)\n",
        "            correct_pred += (predicted_labels == targets).sum()\n",
        "        return correct_pred.float()/num_examples * 100\n",
        "\n",
        "\n",
        "def train(num_epochs, model, optimizer, train_loader, device):\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "\n",
        "            features = features.view(-1, 28*28).to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            logits = model(features)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step() # Update parameters\n",
        "\n",
        "            if not batch_idx % 400:\n",
        "                print('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f'\n",
        "                      % (epoch+1, num_epochs, batch_idx,\n",
        "                          len(train_loader), loss))\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
        "                  epoch+1, num_epochs,\n",
        "                  compute_accuracy(model, train_loader, device)))\n",
        "\n",
        "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
        "\n",
        "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
      ],
      "metadata": {
        "id": "HHBOyCo4KXj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Training"
      ],
      "metadata": {
        "id": "KZKjOhjOLiVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(num_epochs, model_pretrained, optimizer_pretrained, train_loader, DEVICE)\n",
        "print(f'Test accuracy: {compute_accuracy(model_pretrained, test_loader, DEVICE):.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDjxvjrAKeFY",
        "outputId": "ed3c998c-54bd-40cb-9dd7-90af64247a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/010 | Batch 000/938 | Loss: 2.2971\n",
            "Epoch: 001/010 | Batch 400/938 | Loss: 0.1774\n",
            "Epoch: 001/010 | Batch 800/938 | Loss: 0.1849\n",
            "Epoch: 001/010 training accuracy: 94.83%\n",
            "Time elapsed: 0.37 min\n",
            "Epoch: 002/010 | Batch 000/938 | Loss: 0.0912\n",
            "Epoch: 002/010 | Batch 400/938 | Loss: 0.0571\n",
            "Epoch: 002/010 | Batch 800/938 | Loss: 0.0569\n",
            "Epoch: 002/010 training accuracy: 97.30%\n",
            "Time elapsed: 0.67 min\n",
            "Epoch: 003/010 | Batch 000/938 | Loss: 0.0802\n",
            "Epoch: 003/010 | Batch 400/938 | Loss: 0.0549\n",
            "Epoch: 003/010 | Batch 800/938 | Loss: 0.0249\n",
            "Epoch: 003/010 training accuracy: 97.94%\n",
            "Time elapsed: 0.98 min\n",
            "Epoch: 004/010 | Batch 000/938 | Loss: 0.0687\n",
            "Epoch: 004/010 | Batch 400/938 | Loss: 0.1166\n",
            "Epoch: 004/010 | Batch 800/938 | Loss: 0.1479\n",
            "Epoch: 004/010 training accuracy: 98.34%\n",
            "Time elapsed: 1.28 min\n",
            "Epoch: 005/010 | Batch 000/938 | Loss: 0.0716\n",
            "Epoch: 005/010 | Batch 400/938 | Loss: 0.1389\n",
            "Epoch: 005/010 | Batch 800/938 | Loss: 0.0586\n",
            "Epoch: 005/010 training accuracy: 98.12%\n",
            "Time elapsed: 1.59 min\n",
            "Epoch: 006/010 | Batch 000/938 | Loss: 0.1678\n",
            "Epoch: 006/010 | Batch 400/938 | Loss: 0.2201\n",
            "Epoch: 006/010 | Batch 800/938 | Loss: 0.0299\n",
            "Epoch: 006/010 training accuracy: 98.23%\n",
            "Time elapsed: 1.88 min\n",
            "Epoch: 007/010 | Batch 000/938 | Loss: 0.0607\n",
            "Epoch: 007/010 | Batch 400/938 | Loss: 0.1654\n",
            "Epoch: 007/010 | Batch 800/938 | Loss: 0.0948\n",
            "Epoch: 007/010 training accuracy: 98.58%\n",
            "Time elapsed: 2.19 min\n",
            "Epoch: 008/010 | Batch 000/938 | Loss: 0.0647\n",
            "Epoch: 008/010 | Batch 400/938 | Loss: 0.0813\n",
            "Epoch: 008/010 | Batch 800/938 | Loss: 0.0907\n",
            "Epoch: 008/010 training accuracy: 98.61%\n",
            "Time elapsed: 2.50 min\n",
            "Epoch: 009/010 | Batch 000/938 | Loss: 0.0031\n",
            "Epoch: 009/010 | Batch 400/938 | Loss: 0.0082\n",
            "Epoch: 009/010 | Batch 800/938 | Loss: 0.0114\n",
            "Epoch: 009/010 training accuracy: 98.90%\n",
            "Time elapsed: 2.82 min\n",
            "Epoch: 010/010 | Batch 000/938 | Loss: 0.0338\n",
            "Epoch: 010/010 | Batch 400/938 | Loss: 0.0390\n",
            "Epoch: 010/010 | Batch 800/938 | Loss: 0.0008\n",
            "Epoch: 010/010 training accuracy: 99.13%\n",
            "Time elapsed: 3.13 min\n",
            "Total Training Time: 3.13 min\n",
            "Test accuracy: 97.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multilayer Perceptron with LoRA and DoRA"
      ],
      "metadata": {
        "id": "SsokAzxkLs1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
        "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
        "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.alpha * (x @ self.A @ self.B)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LinearWithLoRA(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        lora = self.lora.A @ self.lora.B\n",
        "        combined_weight = self.linear.weight + self.lora.alpha*lora.T\n",
        "        return F.linear(x, combined_weight, self.linear.bias)\n",
        "\n",
        "\n",
        "class LinearWithDoRA(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "\n",
        "        self.m = nn.Parameter(\n",
        "            self.linear.weight.norm(p=2, dim=0, keepdim=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        lora = self.lora.A @ self.lora.B\n",
        "        numerator = self.linear.weight + self.lora.alpha*lora.T\n",
        "        denominator = numerator.norm(p=2, dim=0, keepdim=True)\n",
        "        directional_component = numerator / denominator\n",
        "        new_weight = self.m * directional_component\n",
        "        return F.linear(x, new_weight, self.linear.bias)"
      ],
      "metadata": {
        "id": "_Q58OBuUKf0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "layer = nn.Linear(10, 2)\n",
        "x = torch.randn((1, 10))\n",
        "\n",
        "print(\"Original output:\", layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11HZMkdzL3m3",
        "outputId": "c6bfdbab-5bc2-417a-c7a0-3512cb600087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original output: tensor([[0.6639, 0.4487]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_lora_2 = LinearWithLoRA(layer, rank=2, alpha=4)\n",
        "print(\"LoRA output:\", layer_lora_2(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li20pubyL7SR",
        "outputId": "a69d5831-8049-4134-80d4-ed04445600f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA output: tensor([[0.6639, 0.4487]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_dora_2 = LinearWithDoRA(layer, rank=2, alpha=4)\n",
        "\n",
        "print(\"DoRA output:\", layer_dora_2(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bs7OwgKhMFlU",
        "outputId": "8c6b2c1f-e259-4584-b9c3-ef9ff07777e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DoRA output: tensor([[0.6639, 0.4487]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pretrained"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go5QxuQQMJbK",
        "outputId": "f11067eb-c3b7-41be-b9c2-b87b7ac138ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultilayerPerceptron(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "model_lora = copy.deepcopy(model_pretrained)\n",
        "model_dora = copy.deepcopy(model_pretrained)"
      ],
      "metadata": {
        "id": "ZG5ykq7SMLa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lora.layers[0] = LinearWithLoRA(model_lora.layers[0], rank=4, alpha=8)\n",
        "model_lora.layers[2] = LinearWithLoRA(model_lora.layers[2], rank=4, alpha=8)\n",
        "model_lora.layers[4] = LinearWithLoRA(model_lora.layers[4], rank=4, alpha=8)\n",
        "\n",
        "model_lora.to(DEVICE)\n",
        "optimizer_lora = torch.optim.Adam(model_lora.parameters(), lr=learning_rate)\n",
        "model_lora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8soI4WtuMQyN",
        "outputId": "690fd7d0-c0d0-4716-deb2-c6e22cd3ec5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultilayerPerceptron(\n",
              "  (layers): Sequential(\n",
              "    (0): LinearWithLoRA(\n",
              "      (linear): Linear(in_features=784, out_features=128, bias=True)\n",
              "      (lora): LoRALayer()\n",
              "    )\n",
              "    (1): ReLU()\n",
              "    (2): LinearWithLoRA(\n",
              "      (linear): Linear(in_features=128, out_features=256, bias=True)\n",
              "      (lora): LoRALayer()\n",
              "    )\n",
              "    (3): ReLU()\n",
              "    (4): LinearWithLoRA(\n",
              "      (linear): Linear(in_features=256, out_features=10, bias=True)\n",
              "      (lora): LoRALayer()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dora.layers[0] = LinearWithDoRA(model_dora.layers[0], rank=4, alpha=8)\n",
        "model_dora.layers[2] = LinearWithDoRA(model_dora.layers[2], rank=4, alpha=8)\n",
        "model_dora.layers[4] = LinearWithDoRA(model_dora.layers[4], rank=4, alpha=8)\n",
        "\n",
        "model_dora.to(DEVICE)\n",
        "optimizer_dora = torch.optim.Adam(model_dora.parameters(), lr=learning_rate)\n",
        "model_dora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k94ZWFRZMViN",
        "outputId": "d16b0c2e-beef-40f8-f3f1-20f7b9a906eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultilayerPerceptron(\n",
              "  (layers): Sequential(\n",
              "    (0): LinearWithDoRA(\n",
              "      (linear): Linear(in_features=784, out_features=128, bias=True)\n",
              "      (lora): LoRALayer()\n",
              "    )\n",
              "    (1): ReLU()\n",
              "    (2): LinearWithDoRA(\n",
              "      (linear): Linear(in_features=128, out_features=256, bias=True)\n",
              "      (lora): LoRALayer()\n",
              "    )\n",
              "    (3): ReLU()\n",
              "    (4): LinearWithDoRA(\n",
              "      (linear): Linear(in_features=256, out_features=10, bias=True)\n",
              "      (lora): LoRALayer()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Test accuracy original model: {compute_accuracy(model_pretrained, test_loader, DEVICE):.2f}%')\n",
        "print(f'Test accuracy LoRA model: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "print(f'Test accuracy DoRA model: {compute_accuracy(model_dora, test_loader, DEVICE):.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6bzUOGuMaXI",
        "outputId": "f7f2c175-4461-4ddd-e163-e996fb228868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy original model: 97.59%\n",
            "Test accuracy LoRA model: 97.59%\n",
            "Test accuracy DoRA model: 97.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetuning With LoRA"
      ],
      "metadata": {
        "id": "AHRsCVvsNdna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_linear_layers(model):\n",
        "    for child in model.children():\n",
        "        if isinstance(child, nn.Linear):\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            # Recursively freeze linear layers in children modules\n",
        "            freeze_linear_layers(child)"
      ],
      "metadata": {
        "id": "6WRrdC2sMgXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freeze_linear_layers(model_lora)\n",
        "\n",
        "for name, param in model_lora.named_parameters():\n",
        "    print(f\"{name}: {param.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX-mUPuPNjW1",
        "outputId": "00a14203-5de2-4716-b6cd-da502de0d7dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.linear.weight: False\n",
            "layers.0.linear.bias: False\n",
            "layers.0.lora.A: True\n",
            "layers.0.lora.B: True\n",
            "layers.2.linear.weight: False\n",
            "layers.2.linear.bias: False\n",
            "layers.2.lora.A: True\n",
            "layers.2.lora.B: True\n",
            "layers.4.linear.weight: False\n",
            "layers.4.linear.bias: False\n",
            "layers.4.lora.A: True\n",
            "layers.4.lora.B: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_lora = torch.optim.Adam(model_lora.parameters(), lr=learning_rate)\n",
        "train(num_epochs, model_lora, optimizer_lora, train_loader, DEVICE)\n",
        "print(f'Test accuracy LoRA finetune: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoJqSDZiNkhg",
        "outputId": "d8b8c604-d31d-43df-af16-2b0aa31347d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/010 | Batch 000/938 | Loss: 0.0032\n",
            "Epoch: 001/010 | Batch 400/938 | Loss: 0.0098\n",
            "Epoch: 001/010 | Batch 800/938 | Loss: 0.2822\n",
            "Epoch: 001/010 training accuracy: 98.81%\n",
            "Time elapsed: 0.30 min\n",
            "Epoch: 002/010 | Batch 000/938 | Loss: 0.1740\n",
            "Epoch: 002/010 | Batch 400/938 | Loss: 0.0479\n",
            "Epoch: 002/010 | Batch 800/938 | Loss: 0.0421\n",
            "Epoch: 002/010 training accuracy: 98.90%\n",
            "Time elapsed: 0.62 min\n",
            "Epoch: 003/010 | Batch 000/938 | Loss: 0.0000\n",
            "Epoch: 003/010 | Batch 400/938 | Loss: 0.0330\n",
            "Epoch: 003/010 | Batch 800/938 | Loss: 0.0224\n",
            "Epoch: 003/010 training accuracy: 99.24%\n",
            "Time elapsed: 0.92 min\n",
            "Epoch: 004/010 | Batch 000/938 | Loss: 0.1600\n",
            "Epoch: 004/010 | Batch 400/938 | Loss: 0.0220\n",
            "Epoch: 004/010 | Batch 800/938 | Loss: 0.0485\n",
            "Epoch: 004/010 training accuracy: 99.21%\n",
            "Time elapsed: 1.23 min\n",
            "Epoch: 005/010 | Batch 000/938 | Loss: 0.0037\n",
            "Epoch: 005/010 | Batch 400/938 | Loss: 0.0007\n",
            "Epoch: 005/010 | Batch 800/938 | Loss: 0.0959\n",
            "Epoch: 005/010 training accuracy: 98.77%\n",
            "Time elapsed: 1.53 min\n",
            "Epoch: 006/010 | Batch 000/938 | Loss: 0.0001\n",
            "Epoch: 006/010 | Batch 400/938 | Loss: 0.0535\n",
            "Epoch: 006/010 | Batch 800/938 | Loss: 0.0031\n",
            "Epoch: 006/010 training accuracy: 99.35%\n",
            "Time elapsed: 1.84 min\n",
            "Epoch: 007/010 | Batch 000/938 | Loss: 0.0003\n",
            "Epoch: 007/010 | Batch 400/938 | Loss: 0.0000\n",
            "Epoch: 007/010 | Batch 800/938 | Loss: 0.0004\n",
            "Epoch: 007/010 training accuracy: 98.89%\n",
            "Time elapsed: 2.13 min\n",
            "Epoch: 008/010 | Batch 000/938 | Loss: 0.0687\n",
            "Epoch: 008/010 | Batch 400/938 | Loss: 0.0096\n",
            "Epoch: 008/010 | Batch 800/938 | Loss: 0.0869\n",
            "Epoch: 008/010 training accuracy: 99.16%\n",
            "Time elapsed: 2.44 min\n",
            "Epoch: 009/010 | Batch 000/938 | Loss: 0.1453\n",
            "Epoch: 009/010 | Batch 400/938 | Loss: 0.0017\n",
            "Epoch: 009/010 | Batch 800/938 | Loss: 0.0158\n",
            "Epoch: 009/010 training accuracy: 99.36%\n",
            "Time elapsed: 2.74 min\n",
            "Epoch: 010/010 | Batch 000/938 | Loss: 0.0223\n",
            "Epoch: 010/010 | Batch 400/938 | Loss: 0.0000\n",
            "Epoch: 010/010 | Batch 800/938 | Loss: 0.0027\n",
            "Epoch: 010/010 training accuracy: 98.75%\n",
            "Time elapsed: 3.04 min\n",
            "Total Training Time: 3.04 min\n",
            "Test accuracy LoRA finetune: 96.98%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetuning With DoRA"
      ],
      "metadata": {
        "id": "9xbpkxfTPNRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freeze_linear_layers(model_dora)\n",
        "\n",
        "for name, param in model_dora.named_parameters():\n",
        "    print(f\"{name}: {param.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyzf5w0SNpSn",
        "outputId": "74327415-766f-4180-d3b4-7da18584fc0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.m: True\n",
            "layers.0.linear.weight: False\n",
            "layers.0.linear.bias: False\n",
            "layers.0.lora.A: True\n",
            "layers.0.lora.B: True\n",
            "layers.2.m: True\n",
            "layers.2.linear.weight: False\n",
            "layers.2.linear.bias: False\n",
            "layers.2.lora.A: True\n",
            "layers.2.lora.B: True\n",
            "layers.4.m: True\n",
            "layers.4.linear.weight: False\n",
            "layers.4.linear.bias: False\n",
            "layers.4.lora.A: True\n",
            "layers.4.lora.B: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_dora = torch.optim.Adam(model_dora.parameters(), lr=learning_rate)\n",
        "train(num_epochs, model_dora, optimizer_dora, train_loader, DEVICE)\n",
        "print(f'Test accuracy DoRA finetune: {compute_accuracy(model_dora, test_loader, DEVICE):.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4-xIa1jPRnF",
        "outputId": "52503e47-aa23-466b-de8e-db84bd7927f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/010 | Batch 000/938 | Loss: 0.0016\n",
            "Epoch: 001/010 | Batch 400/938 | Loss: 0.0490\n",
            "Epoch: 001/010 | Batch 800/938 | Loss: 0.0824\n",
            "Epoch: 001/010 training accuracy: 99.02%\n",
            "Time elapsed: 0.35 min\n",
            "Epoch: 002/010 | Batch 000/938 | Loss: 0.0242\n",
            "Epoch: 002/010 | Batch 400/938 | Loss: 0.0046\n",
            "Epoch: 002/010 | Batch 800/938 | Loss: 0.0760\n",
            "Epoch: 002/010 training accuracy: 99.36%\n",
            "Time elapsed: 0.71 min\n",
            "Epoch: 003/010 | Batch 000/938 | Loss: 0.0583\n",
            "Epoch: 003/010 | Batch 400/938 | Loss: 0.0264\n",
            "Epoch: 003/010 | Batch 800/938 | Loss: 0.0269\n",
            "Epoch: 003/010 training accuracy: 99.36%\n",
            "Time elapsed: 1.06 min\n",
            "Epoch: 004/010 | Batch 000/938 | Loss: 0.0051\n",
            "Epoch: 004/010 | Batch 400/938 | Loss: 0.0032\n",
            "Epoch: 004/010 | Batch 800/938 | Loss: 0.0023\n",
            "Epoch: 004/010 training accuracy: 99.33%\n",
            "Time elapsed: 1.42 min\n",
            "Epoch: 005/010 | Batch 000/938 | Loss: 0.0195\n",
            "Epoch: 005/010 | Batch 400/938 | Loss: 0.0304\n",
            "Epoch: 005/010 | Batch 800/938 | Loss: 0.0529\n",
            "Epoch: 005/010 training accuracy: 99.38%\n",
            "Time elapsed: 1.77 min\n",
            "Epoch: 006/010 | Batch 000/938 | Loss: 0.0013\n",
            "Epoch: 006/010 | Batch 400/938 | Loss: 0.0260\n",
            "Epoch: 006/010 | Batch 800/938 | Loss: 0.0977\n",
            "Epoch: 006/010 training accuracy: 99.15%\n",
            "Time elapsed: 2.13 min\n",
            "Epoch: 007/010 | Batch 000/938 | Loss: 0.0103\n",
            "Epoch: 007/010 | Batch 400/938 | Loss: 0.0054\n",
            "Epoch: 007/010 | Batch 800/938 | Loss: 0.1028\n",
            "Epoch: 007/010 training accuracy: 99.35%\n",
            "Time elapsed: 2.49 min\n",
            "Epoch: 008/010 | Batch 000/938 | Loss: 0.0018\n",
            "Epoch: 008/010 | Batch 400/938 | Loss: 0.0035\n",
            "Epoch: 008/010 | Batch 800/938 | Loss: 0.0030\n",
            "Epoch: 008/010 training accuracy: 99.25%\n",
            "Time elapsed: 2.86 min\n",
            "Epoch: 009/010 | Batch 000/938 | Loss: 0.0135\n",
            "Epoch: 009/010 | Batch 400/938 | Loss: 0.0662\n",
            "Epoch: 009/010 | Batch 800/938 | Loss: 0.0010\n",
            "Epoch: 009/010 training accuracy: 99.35%\n",
            "Time elapsed: 3.21 min\n",
            "Epoch: 010/010 | Batch 000/938 | Loss: 0.0238\n",
            "Epoch: 010/010 | Batch 400/938 | Loss: 0.0440\n",
            "Epoch: 010/010 | Batch 800/938 | Loss: 0.0300\n",
            "Epoch: 010/010 training accuracy: 99.28%\n",
            "Time elapsed: 3.57 min\n",
            "Total Training Time: 3.57 min\n",
            "Test accuracy DoRA finetune: 97.62%\n"
          ]
        }
      ]
    }
  ]
}